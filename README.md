# Twinly API

Eine saubere FastAPI-Anwendung f√ºr LLM-basierte Chatbots, Workflows und Agents.

## üèóÔ∏è Projekt-Struktur

```
/app
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ main.py                 ‚Üê FastAPI App Initialisierung
‚îú‚îÄ‚îÄ /api                    ‚Üê FastAPI Routen
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ chat.py             ‚Üê Chat Completion Endpoints
‚îú‚îÄ‚îÄ /logic                  ‚Üê Agenten, Tools, KI-Logik
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ agent_controller.py ‚Üê Plan-Reason-Respond Agent
‚îú‚îÄ‚îÄ /models                 ‚Üê Pydantic Models
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ chat.py             ‚Üê Chat-bezogene Models
‚îú‚îÄ‚îÄ /services               ‚Üê Externe Service-Integrationen
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ memory_qdrant.py    ‚Üê Qdrant Vector Database Service
‚îÇ   ‚îî‚îÄ‚îÄ /llms               ‚Üê LLM Provider (neue Struktur)
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ base_llm_provider.py      ‚Üê Abstrakte Basisklasse
‚îÇ       ‚îú‚îÄ‚îÄ azure_openai_provider.py  ‚Üê Azure OpenAI Provider
‚îÇ       ‚îú‚îÄ‚îÄ claude_provider.py        ‚Üê Anthropic Claude Provider
‚îÇ       ‚îî‚îÄ‚îÄ echo_provider.py          ‚Üê Echo Provider (Demo)
‚îî‚îÄ‚îÄ /utils                  ‚Üê Hilfsfunktionen
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îî‚îÄ‚îÄ logging.py          ‚Üê Logging-Konfiguration
```

## ü§ñ Agent-Mode Features

Der **agent-mode** implementiert ein fortgeschrittenes "plan ‚Äì reason ‚Äì respond"-Paradigma:

1. **üéØ Plan**: Bestimmt das Ziel basierend auf der Benutzereingabe
2. **üß† Reason**: Generiert Chain-of-Thought-Reasoning
3. **üí¨ Respond**: Erstellt finale Antwort basierend auf Planung und √úberlegung

## üöÄ Verf√ºgbare Modelle

- **`agent-mode`** - Fortgeschrittener Agent mit plan-reason-respond Paradigma
- **`gpt4.1-chat`** - Azure OpenAI GPT-4 (falls konfiguriert)
- **`claude-4-sonnet`** - Anthropic Claude (falls konfiguriert)
- **`echo-model`** - Einfaches Echo-Modell zum Testen

## üîß Installation und Setup

### 1. Environment aktivieren
```bash
conda activate twinly
```

### 2. Dependencies installieren
```bash
pip install -r requirements.txt
```

### 3. Environment-Variablen konfigurieren
```bash
# Azure OpenAI (optional)
export AZURE_OPENAI_ENDPOINT="your_endpoint"
export AZURE_OPENAI_API_KEY="your_key"
export AZURE_OPENAI_API_VERSION="2024-02-15-preview"
export AZURE_OPENAI_DEPLOYMENT_NAME="gpt-4"

# Claude/Anthropic (optional)
export CLAUDE_API_KEY="your_claude_key"

# Logging Level
export LOG_LEVEL="INFO"
```

### 4. Anwendung starten
```bash
# WICHTIG: Immer vom Projekt-Root-Verzeichnis ausf√ºhren!
cd /pfad/zu/twinly
conda activate twinly
uvicorn app.main:app --reload
```

## üîß Lokale Entwicklung

### Wichtige Hinweise f√ºr die lokale Entwicklung

**‚ö†Ô∏è H√§ufiger Fehler vermeiden:**
- F√ºhre uvicorn **NIEMALS** aus dem `/app` Verzeichnis aus
- Das f√ºhrt zu `ModuleNotFoundError: No module named 'app'`

**‚úÖ Korrekte Vorgehensweise:**
```bash
# 1. Zum Projekt-Root navigieren
cd /path/to/twinly

# 2. Conda Environment aktivieren
conda activate twinly

# 3. Server starten (WICHTIG: app.main:app als Modul-Pfad verwenden)
uvicorn app.main:app --reload
```

**Warum dieser Ansatz?**
- Die Anwendung verwendet absolute Imports (`from app.api.chat import ...`)
- Python muss das `app` Package vom Root-Verzeichnis aus finden k√∂nnen
- Der Modul-Pfad `app.main:app` zeigt auf die FastAPI-Instanz in `app/main.py`

**Entwicklung Workflow:**
1. **Terminal 1**: Server starten mit obigem Befehl
2. **Terminal 2**: Tests ausf√ºhren, Dependencies installieren, etc.
3. **Code √§ndern**: Auto-reload funktioniert automatisch
4. **API testen**: http://127.0.0.1:8000 oder http://127.0.0.1:8000/docs

**Debugging:**
- Server l√§uft auf: http://127.0.0.1:8000
- API Dokumentation: http://127.0.0.1:8000/docs
- Health Check: http://127.0.0.1:8000/health
- Logs werden in der Konsole angezeigt

## üß™ Tests ausf√ºhren

```bash
# Alle Tests
python -m pytest -v

# Nur Main-API Tests
python -m pytest test_main.py -v

# Nur Agent Tests
python -m pytest test_agent.py -v
```

## üì° API Endpoints

### GET `/v1/models`
Liste verf√ºgbare Modelle auf

```bash
curl -X GET "http://127.0.0.1:8000/v1/models"
```

### POST `/v1/chat/completions`
Erstelle Chat Completions (OpenAI kompatibel)

```bash
curl -X POST "http://127.0.0.1:8000/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "agent-mode",
    "messages": [
      {"role": "user", "content": "Wie lerne ich Python?"}
    ],
    "stream": false
  }'
```

### Streaming Support
```bash
curl -X POST "http://127.0.0.1:8000/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "echo-model",
    "messages": [
      {"role": "user", "content": "Test streaming"}
    ],
    "stream": true
  }'
```

**Hinweis**: Streaming wird f√ºr `agent-mode` nicht unterst√ºtzt.

### GET `/health`
Health Check Endpoint

### GET `/`
Informative Homepage mit API-Dokumentation

### GET `/docs`
Automatische Swagger UI Dokumentation

## üîç Debugging und Logging

Die Anwendung verwendet strukturiertes Logging:

```python
from app.utils.logging import get_logger

logger = get_logger(__name__)
logger.info("Your log message")
```

Log-Level kann √ºber die `LOG_LEVEL` Environment-Variable gesteuert werden:
- `DEBUG`
- `INFO` (Standard)
- `WARNING`
- `ERROR`
- `CRITICAL`

## üèóÔ∏è Entwicklung

### Neue Agents hinzuf√ºgen
1. Erstelle eine neue Datei in `app/logic/`
2. Implementiere deine Agent-Logik
3. Registriere den Agent in `app/api/chat.py`
4. F√ºge das Modell zur Models-Liste hinzu

### Neue Services hinzuf√ºgen
1. Erstelle eine neue Service-Klasse in `app/services/`
2. Implementiere die erforderlichen Methoden
3. Integriere den Service in den entsprechenden Router

### Neue LLM Provider hinzuf√ºgen
1. Erstelle eine neue Provider-Klasse in `app/services/llms/` die von `BaseLLMProvider` erbt
2. Implementiere alle abstrakten Methoden (`is_available`, `generate_response`, `generate_streaming_response`)
3. Erstelle eine globale Instanz am Ende der Provider-Datei
4. Aktualisiere das `MODEL_MAPPING` in `app/api/chat.py`
5. F√ºge das Modell zur Models-Liste im `/v1/models` Endpoint hinzu
6. Schreibe Contract- und Mock-Tests f√ºr den Provider in `tests/`

### Neue Models hinzuf√ºgen
1. Definiere Pydantic Models in `app/models/`
2. Verwende Type Hints f√ºr alle Funktionen
3. Validiere Ein- und Ausgaben

## ‚ö° Performance

- Async/Await f√ºr alle I/O-Operationen
- Strukturierte Logging f√ºr besseres Debugging
- Modulare Architektur f√ºr einfache Wartung
- OpenAI-kompatible API f√ºr nahtlose Integration

## üîí Sicherheit

- Keine sensiblen Daten in Code committen
- Environment-Variablen f√ºr API-Schl√ºssel verwenden
- Input-Validation √ºber Pydantic Models
- Error Handling mit detailliertem Logging

## üìù Konventionen

- Verwende nur `FastAPI` und `uvicorn`
- Verwende NICHT die lokale Python-Umgebung, sondern immer `conda activate twinly`
- Halte alles so minimal wie m√∂glich
- Verwende keine unn√∂tigen Dependencies
- Schreibe klaren, gut kommentierten Code
- API-Antworten sind immer im JSON-Format
- Schreibe Tests f√ºr alles vor der Implementierung neuer Features
- Verwende Type Hints f√ºr alle Funktions-Signaturen
- Verwende `pydantic` Models f√ºr Request- und Response-Validation
- Verwende `pytest` f√ºr Tests

## üö´ Vermeiden

- Keine komplexen Features wie Authentifizierung oder Datenbanken
- Keine externen Tools oder Frameworks au√üer den genannten

## How to add a new LLM provider

Alle LLM-Provider nutzen jetzt eine einheitliche Architektur basierend auf der abstrakten `BaseLLMProvider`-Klasse:

### Schritte:
1. **Provider-Klasse erstellen**: Erstelle eine neue Datei in `app/services/llms/my_provider.py`
2. **Von BaseLLMProvider erben**: Implementiere alle abstrakten Methoden:
   - `is_available() -> bool`
   - `async generate_response(req: ChatCompletionRequest) -> Dict[str, Any]`  
   - `async generate_streaming_response(req: ChatCompletionRequest) -> AsyncGenerator[str, None]`
3. **Globale Instanz**: Erstelle eine globale Instanz am Ende der Datei: `my_provider = MyProvider()`
4. **Export hinzuf√ºgen**: F√ºge den Provider zu `app/services/llms/__init__.py` hinzu
5. **Model Mapping**: Aktualisiere `MODEL_MAPPING` in `app/api/chat.py`
6. **Models Endpoint**: F√ºge das Modell zum `/v1/models` Endpoint hinzu
7. **Tests schreiben**: Erstelle Contract- und Mock-Tests in `tests/`
8. **Linting & Type Checks**: Stelle sicher, dass `ruff`, `black` und `mypy` bestehen
9. **Dokumentation**: Dokumentiere erforderliche Environment-Variablen

### Beispiel:
```python
# app/services/llms/my_provider.py
from .base_llm_provider import BaseLLMProvider

class MyProvider(BaseLLMProvider):
    def is_available(self) -> bool:
        return True  # Check API key, etc.
    
    async def generate_response(self, req: ChatCompletionRequest):
        # Implementation here
        pass
    
    async def generate_streaming_response(self, req: ChatCompletionRequest):
        # Implementation here  
        yield "data: chunk\n\n"

# Global instance
my_provider = MyProvider()
```

### Vorteile der neuen Architektur:
- **Einheitliche API**: Alle Provider implementieren dieselben Methoden
- **Austauschbarkeit**: Provider k√∂nnen einfach ausgetauscht werden
- **Testbarkeit**: Jeder Provider kann isoliert getestet werden
- **Erweiterbarkeit**: Neue Provider sind einfach hinzuzuf√ºgen
- **Factory Pattern**: `MODEL_MAPPING` erm√∂glicht dynamische Provider-Auswahl
